---
title: "Meta-analysis of prediction model performance"
author:
  - name: Thomas Debray
    orcid: 0000-0002-1790-2719
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Meta-analysis of prediction model performance}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: "https://api.citedrive.com/bib/0d25b38b-db8f-43c4-b934-f4e2f3bd655a/references.bib?x=eyJpZCI6ICIwZDI1YjM4Yi1kYjhmLTQzYzQtYjkzNC1mNGUyZjNiZDY1NWEiLCAidXNlciI6ICIyNTA2IiwgInNpZ25hdHVyZSI6ICI0MGFkYjZhMzYyYWE5Y2U0MjQ2NWE2ZTQzNjlhMWY3NTk5MzhhNzUxZDNjYWIxNDlmYjM4NDgwOTYzMzY5YzFlIn0=/bibliography.bib"
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = FALSE,
  message = FALSE,
  warning = FALSE,
  tidy = TRUE
)
```

```{r, echo = F}
library(knitr)
library(dplyr)

# usethis::use_pkgdown_github_pages().
# pkgdown::build_site()
```

# Case Study
The [EuroSCORE II](http://www.euroscore.org/calc.html) is a commonly used scoring rule for estimating the risk of in-hospital mortality in  patients undergoing major cardiac surgery. It was developed using data from  16,828 adult patients from 43 countries. Predictors include patient characteristics (e.g. age, gender), cardiac related factors (e.g. recent MI) and surgery related factors (e.g. Surgery on thoracic aorta). In 2014, a systematic review was undertaken by @guida_performance_2014 to search articles assessing the performance of EuroSCORE II on perioperative mortality in cardiac surgery. The systematic review identified 24 eligible validation studies, 22 studies were included in the main analysis. 

In this case study, we summarize the results from these 22 studies, as well as the results from the split-sample validation contained within original development article of EuroSCORE II. We will use the **metamisc** package to derive summary estimates for the discrimination and calibration performance of EuroSCORE II, to evaluate the presence of between-study heterogeneity, and to identify potential sources of between-study heterogeneity. A step-by-step tutorial is provided by @debray_guide_2017. 

We can load the data from all 23 validation studies as follows:

```{r}
library(metamisc)
data(EuroSCORE)
```

```{r, echo = F}
out <- EuroSCORE %>% mutate(mortality = sprintf("%.2f", 100*n.events/n),
                            euroscore = sprintf("%.2f", 100*Pe)) %>%
  select(Study, n, mortality, euroscore, c.index)
kable(out, col.names = c("Study", "Patients (n)", "Mortality (%)", "euroSCORE II (%)", "c-index"),
      align = "lrcc")
```


# Meta-analysis of calibration performance
Calibration refers to a model's accuracy of predicted risk probabilities, and indicates the extent to which expected outcomes (predicted from the model) and observed outcomes agree. Summarizing estimates of calibration performance is challenging because calibration plots are most often not presented, and because studies tend to report different types of summary statistics in calibration. For example, in the case study, calibration was assessed using the Hosmer-Lemeshow test, calibration plots or by comparing the observed mortality to the predicted EuroSCORE II (either overall or for groups of patients). Within each validation study, we can compare the total	number	of observed events (O) with the total number of expected (predicted)	events by deriving their ratio O:E. The total O:E ratio provides a rough indication of the overall model calibration (across the entire range of predicted risks). It	describes	whether	more	(O:E	>	1)	or	fewer	(O:E	<	1)	events	occurred		than	expected	 based	on	the	model. Whilst the O:E ratio itself was not explicitly reported in all studies, it can be calculated from other reported information:

```{r}
EuroSCORE <- EuroSCORE %>% mutate(oe = n.events/e.events)
```

The O:E ratio can also be derived from the observed and predicted mortality risk `Po` and, respectively, `Pe`:

```{r, eval = F}
EuroSCORE %>% select(Po, Pe) %>% mutate(oe = Po/Pe)
```

It is recommended to first transform extracted O:E ratios to	the	log (natural logarithm) scale before applying a meta-analysis [@snell_meta-analysis_2017].

```{r}
EuroSCORE <- EuroSCORE %>% mutate(logOE = log(oe))
```

Consequently, we need to derive the standard error of the log O:E ratio, which is approximately given as 

$$\sqrt{(1/O) - (1/N)} \approx \sqrt{1/O}$$

We can implement this as follows:

```{r}
EuroSCORE <- EuroSCORE %>% mutate(se.logOE = sqrt((1/n.events - 1/n)))
```

We can now visualise the calibration performance of EuroSCORE II across the included studies by generating a forest plot:

```{r, echo = FALSE}
metafor::forest(x = EuroSCORE$logOE, sei = EuroSCORE$se.logOE, slab = EuroSCORE$Study, 
                transf = exp, xlab = "O:E ratio", refline = 1, cex = 0.8)
```

For each validation study, the O:E ratio is provided with its 95% confidence interval. The size of the boxes indicates the	sample size of the corresponding validation study. In this forest plot, O:E	ratios appear to vary	substantially	across the included validation	studies. These results suggest that the calibration of EuroSCORE II is prone to substantial between-study heterogeneity.

In [metamisc](https://CRAN.R-project.org/package=metamisc), we can combine the aforementioned steps as follows:

```{r}
oe.ad <- oecalc(N = n, O = n.events, E = e.events, slab = Study, 
                data = EuroSCORE)
plot(oe.ad, refline = 1, sort = "no")
```

Note that if we want to derive the log O:E ratio instead, we should use:

```{r}
logoe.ad <- oecalc(N = n, O = n.events, E = e.events, slab = Study, g = "log(OE)", data = EuroSCORE)
```




# References
