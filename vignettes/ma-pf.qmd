---
title: "Meta-analysis of prognostic factors"
author:
  - name: Thomas Debray
    orcid: 0000-0002-1790-2719
date: last-modified
date-format: "[Last Updated on] DD MMMM, YYYY"
params:
  dir_data: "data"
  dir_r: "R_TD"
format:
  html:
    toc: true
    number-sections: true
    css: styles.css
bibliography: "https://api.citedrive.com/bib/093a08ac-0337-479e-a147-17929fa7f7b0/references.bib?x=eyJpZCI6ICIwOTNhMDhhYy0wMzM3LTQ3OWUtYTE0Ny0xNzkyOWZhN2Y3YjAiLCAidXNlciI6ICI0MDA5IiwgInNpZ25hdHVyZSI6ICIwOWJiZjg2ZDJiNjk1NmQ1ODVhZTMxZmZiODI0OTgyNWYxYTEzNTQ5MzdiZjBiNzJiZjY3MTlhMTE2NGJhNzQ3In0=/bibliography.bib"
---

```{r}
#| echo: false
#| message: false
#| warning: false
library(knitr)

# Use GitHub actions to automatically build and publish the site every time you make a change. The easiest way to set this up is to run usethis::use_pkgdown_github_pages().

show.text = TRUE
```

# Introduction

An important task in medical research is the identification and assessment of prognostic factors. A prognostic factor is any measure that, among people with a given health condition (that is, a startpoint), is associated with a subsequent clinical outcome (an endpoint) [@riley_prognosis_2013]. Commonly investigated prognostic factors include simple measures such as age, sex, and size of tumor, but they can also include more complex factors such as abnormal levels of proteins or catecholamines and unusual genetic mutations [@sauerbrei_evidence-based_2006]. They can be useful as modifiable targets for interventions to improve outcomes, building blocks for prognostic models, or predictors of differential treatment response.

Over the past few decades, numerous prognostic factor studies have been published in the medical literature. For example, @riley_reporting_2003 identified 260 studies reporting associations for 130 different tumour markers for neuroblastoma. More recently, @tzoulaki_assessment_2009 identified 79 studies reporting the added value of 86 different markers when added to the Framingham risk score. Despite this huge research effort, the prognostic value of most traditional factors under discussion is uncertain and the usefulness of many specific markers, prognostic indices, and classification schemes is still unproven [@sauerbrei_evidence-based_2006].

The aim of this introduction is to illustrate how to summarize the results from multiple prognostic factor studies and how to investigate sources of between-study heterogeneity. 

In this practical we will make use of the R packages `metamisc` and `metafor`. The [https://cran.r-project.org/package=metafor](metafor) package provides a comprehensive collection of functions for conducting meta-analyses in R. The [https://cran.r-project.org/package=metamisc](metamisc) package provides additional functions to facilitate meta-analysis of prognosis studies. We can load these packages as follows:

```{r}
#| message: false
#| warning: false
library(metafor)
library(metamisc)
```

# Meta-analysis of prognostic effect estimates

## Background information

Endometrial cancer (EC) is the fourth most common malignancy in women and the most common gynecologic cancer. Overall, the 5-year survival rates for EC are approximately 78--90% for stage I, 74% for stage II, 36--57% for stage III, and 20% for stage IV. Such poor outcomes raise an urgent requirement that more accurate prognosis and predictive markers should be applied for EC to guide the therapy and monitor the disease progress for individual patients.

Several biological molecules have been proposed as prognostic biomarkers in EC. Among them, hormone receptors such as estrogen receptors (ER), progesterone receptors (PR), and human epidermal growth factor receptor 2 (HER2) are attractive because of their physiological functions. Recently, Zhang \emph{et al.} conducted a systematic review to evaluate the overall risk of these hormone receptors for endometrial cancer survival [@zhang_prognostic_2015]. This review included 16 studies recruiting 1764 patients for HER2. We can load the data from all 16 studies in R as follows:

```{r}
data(Zhang)
```

This creates an object `Zhang` that contains the summary data from 14 studies reporting on overall survival (OS) and from 6 studies reporting on progression-free survival (PFS). As a result, a total of 20 estimates are available for the hazard ratio of HER2:

```{r}
Zhang
```

For each study, estimates of effect were retrieved as follows. The simplest method consisted in the direct collection of HR, odds ratio or risk ratio, and their 95% CI from the original article, with an HR of less than 1 being associated with a better outcome. If not available, the total numbers of observed deaths/cancer recurrences and the numbers of patients in each group were extracted to calculate HR. When data were only available as Kaplan-Meier curves, data were extracted from the graphical survival plots, and estimation of the HR was then performed using the described method.

A total of 14 studies assessed the relation between HER2 and overall survival. The corresponding hazard ratios (HR) are given below:

```{r kableOS}
Zhang
```
 EEC endometrioid endometrial cancer, UPSC uterine papillary serous carcinoma

The hormone receptor HER2 has prognostic value for survival but is prone to substantial between-study heterogeneity. Hazard ratios appear much larger for studies conducted in the USA

The heterogeneity of the population was probably due to the difference in the baseline characteristics of patients (age, tumor stage, race, methodology for assessing HRs expression, or country), the cutoff value of markers, the undergoing treatment, the duration of follow-up, and others. Also, HRs are unadjusted, and therefore highly prone to heterogeneity in patient spectrum.


Note that the total number of studies is indeed 16:

```{r}
length(unique(Zhang$Study))
```

## Exploratory analyses

It is often helpful to display the effect sizes of all studies in a forest plot. The forest plot for overall survival can then be generated as follows. Briefly, we here provide information on the estimated hazard ratios (via the argument `theta`), as well as the bounds of their 95% confidence interval (via `theta.ci.lb` and `theta.ci.ub`). Further, we only include the 14 estimates for overall survival.

```{r,message=F,warning=F,echo=T,eval=F}
ds <- subset(Zhang, outcome=="OS")
with(ds, forest(theta = HR, theta.ci.lb = HR.025, theta.ci.ub = HR.975, 
                theta.slab = Study, xlab = "Hazard ratio of HER2 versus OS", refline = 1))
```

We can also generate the same plot using `metafor`:

```{r,message=F,warning=F,echo=T,eval=T}
metafor::forest(x = Zhang$HR, ci.lb = Zhang$HR.025, ci.ub = Zhang$HR.975, slab = 
                  Zhang$Study, subset = (Zhang$outcome=="OS"),  xlab = 
                  "Hazard ratio of HER2 versus OS", refline = 1)
```

It is easier to assess the summary effect and the presence of statistical heterogeneity.

## Data preparation

To facilitate any further analysis, information on the standard error of the different study effect sizes is needed. Estimates for the standard error can be obtained from the reported 95% confidence intervals [@altman_how_2011]. However, rather than estimating the standard error of the HR, we will calculate the log HR and estimate its corresponding standard error. The main reason is that the HR is a ratio measure, for which the (within-study) sampling variation can be approximated with a Normal distribution if the log scale is used.

The standard error (SE) of the log hazard ratio is simply given as:

$\mathrm{SE}=(\log(u)-\log(l))/(2*1.96)$

where the upper and lower limits of the 95% CI are $u$ and $l$ respectively. In `R`, we have:

```{r,message=F,warning=F,echo=T,eval=T}
Zhang$logHR <- log(Zhang$HR)
Zhang$se.logHR <- (log(Zhang$HR.975)-log(Zhang$HR.025))/(2*qnorm((1-0.05/2)))
head(Zhang)
```

## Assessment of publication bias

The presence of small-study effects is a common threat to systematic reviews and meta-analyses. Small-study effects is a generic term for the phenomenon that sometimes smaller studies show different, often stronger, effects than the large ones [@debray_detecting_2018]. One possible reason is publication bias. Previously, @altman_systematic_2001 argued that it is probable that studies showing a strong (often statistically significant) prognostic ability are more likely to be published. For this reason, it is important to evaluate the potential presence of small-study effects, which can be verified by visual inspection of the funnel plot. In this plot, the estimate of the reported effect size is plotted against a measure of precision or sample size for each included study of the meta-analysis. The premise is that the scatter of plots should reflect a funnel shape, if small-study effects do not exist (provided that effect sizes are not substantially affected by the presence of between-study heterogeneity). However, when small studies are predominately in one direction (usually the direction of larger effect sizes), asymmetry will ensue.

A common approach to construct the funnel plot is to display the individual observed effect sizes on the x-axis against the corresponding standard errors on the y-axis, and to use the fixed effect summary estimate as reference value. In the absence of publication bias and heterogeneity, one would then expect to see the points forming a funnel shape, with the majority of the points falling inside of the pseudo-confidence region of the summary estimate.

```{r,message=F,warning=F,echo=T,eval=T}
res <- rma(yi = logHR, sei = se.logHR, subset = (outcome=="OS"), method = "FE", data = Zhang)
funnel(res, xlab="Log Hazard Ratio")
```

Most study estimates fall within the pseudo-confidence region, hence there appears limited evidence for publication bias.

We can formally test the presence of asymmetry in the funnel plot by evaluating whether there is an association between the estimated standard error and the estimated effect size.

```{r,message=F,warning=F,echo=T,eval=T}
regtest(x = Zhang$logHR, sei = Zhang$se.logHR, subset = (Zhang$outcome=="OS"), 
        model = "lm", predictor = "sei")
```

```{r,message=F,warning=F,echo=F,eval=T}
regfit = regtest(x=Zhang$logHR, sei=Zhang$se.logHR, subset=(Zhang$outcome=="OS"), model="lm", predictor="sei")
```

The P-value for funnel plot asymmetry is given as `r sprintf("%.3f", regfit$pval)`. Note that it is common to use a 10% level of significance because the number of studies in a meta-analysis is usually low and also there is evidence for funnel plot asymmetry, as the P-value is below 0.10.

Funnel plot asymmetry tests can also be performed using [metamisc](https://cran.r-project.org/package=metamisc)

```{r}
ds <- subset(Zhang, outcome=="OS")
regfit <- with(ds, fat(b=logHR, b.se=se.logHR, method="E-FIV"))
regfit
```

Again, we can construct a funnel plot:

```{r}
plot(regfit)
```

Some caution is warranted when interpreting the results for funnel plot asymmetry tests [@debray_detecting_2018]. First, the power to detect asymmetry is often limited because meta-analyses usually do not include many studies. Second, many tests are known to yield inadequate type-I error rates or to suffer from low power. For instance, it has been demonstrated that aforementioned test to evaluate the association between the estimated standard error and effect size tends to yield type-I error rates that are too high. Finally, funnel plot asymmetry may rather be caused by heterogeneity than by publication bias.

Adjust aformentioned regression test to use inverse of the total sample size (rather than the standard error) as predictor.

```{r,message=F,warning=F}
regtest(x = Zhang$logHR, sei = Zhang$se.logHR, ni = Zhang$N, 
        subset = (Zhang$outcome=="OS"), model = "lm", 
        predictor = "ninv")
```

From here onwards, we will assume that the potential for publication bias is neglegible, and proceed with standard meta-analysis methods.

## Meta-analysis of the prognostic value of HER2

Meta-analysis is an option when the identified studies are considered sufficiently robust and comparable, and requires there are at least two estimates of the same statistic across studies. A random effects approach is often essential to allow for unexplained heterogeneity across studies due to differences in their methods, time-scale, populations, cut-points, adjustment factors, and treatments.

A standard random effects meta-analysis combines the study estimates of the statistic of interest (here given as the log HR of HER2) in order to estimate the average effect (denoted by $\mu$) and its standard deviation (denoted by $\tau$) across studies. If $\hat b_i$ and $\mathrm{var}(\hat b_i)$ denote the estimate and its variance in study $i$, then a general random effects meta-analysis model can be specified as:

$\hat b_i \sim N(\mu, \mathrm{var}(\hat b_i) + \tau^2)$

It is common to first estimate the heterogeneity parameter $\tau$ and to use the resulting value to estimate $\mu$. However, such approach does not adequately reflect the error associated with parameter estimation, especially when the number of studies is small. For this reason, alternative estimators have been proposed that simultaneously estimate $\mu$ and $\tau$. Here, we will focus on Restricted Maximum Likelihood Estimation (REML), which is implemented as default in `metafor`.

```{r,message=F,warning=F,echo=T,eval=T}
resREML <- rma(yi = logHR, sei = se.logHR, subset = (outcome=="OS"), 
               method = "REML", slab = Study, data = Zhang)
resREML
# The following approach yields the same results
# rma(yi=logHR, sei=se.logHR, subset=(outcome=="OS"), data=Zhang)
```

`r if(show.text){paste("The summary effect size is ", sprintf("%.3f", resREML$b), " and the between-study standard deviation is ", sprintf("%.3f", sqrt(resREML$tau2)), "")}`

`r if(show.text){paste("The standard error of the summary effect size is ", sprintf("%.3f", sqrt(vcov(resREML))), " and the standard error of the between-study variance is ", sprintf("%.3f", resREML$se.tau2), "")}`

We can calculate the summary HR and the corresponding 95% CI by back-transforming the estimate for $\mu$ and its confidence bounds:

```{r,message=F,warning=F,echo=T,eval=T}
exp(resREML$b)
exp(c(resREML$ci.lb, resREML$ci.ub))
```

Note that the confidence interval for $\hat \mu$ is usually based on a Student T distribution to account for estimation error in $\hat \tau$.

The summary HR of HER2 is statistically significant, so the hormone receptor appears to be predictive on average. In particular, an increased level of HER2 was associated with poorer survival, on average.

We can also obtain the summary estimate and 95% CI for the HR of HER2 by simply using the `predict` function:

```{r,message=F,warning=F,echo=T,eval=T}
predict(resREML, transf = exp)
```

Although the summary result ($\hat \mu$) is usually the main focus of a meta-analysis, this reflects some average across studies and it may be hard to translate to clinical practice when there is large between-study heterogeneity. We can quantify the impact of between-study heterogeneity by constructing a $100(1-\alpha/2)$% prediction interval, which gives the potential true prognostic effect in a new population conditional on the meta-analysis results [@riley_interpretation_2011]. An approximate prediction interval (PI) is given as follows:

$\hat \mu \pm t_{\alpha, N-2} \sqrt{\hat \tau^2 + \hat \sigma^2}$

where $t_{\alpha, N-2}$ is the $100(1-\alpha/2)$% percentile of the t-distribution for $N-2$ degrees of freedom, $N$ is the number of studies, $\hat \sigma$ is the estimated standard error of $\hat \mu$, and $\hat \tau$ is the estimated between-study standard deviation. In `R`, can calculate the approximate 95% PI for $\hat \mu$ as follows:

```{r,message=F,warning=F,echo=T,eval=T}
level <- 0.05
crit <- qt(c(level/2, 1-(level/2)), df = (resREML$k-2))
mu <- resREML$b
tau2 <- resREML$tau2
sigma2 <- vcov(resREML)
mu + crit * c(sqrt(tau2 + sigma2))
```

`r if(show.text){paste("The boundaries of the approximate 95% PI are given as ", sprintf("%.3f", exp(resREML$b + qt(level/2, df=(resREML$k-2)) * sqrt(tau2 + sigma2))), " to ", sprintf("%.3f", exp(resREML$b + qt(1-level/2, df=(resREML$k-2)) * sqrt(tau2 + sigma2))), ".")}`

There is substantial heterogeneity in the prognostic value of HER 2 and its usefulness may be very limited in certain populations.

A possible approach to enhance the interpretation of meta-analysis results is to calculate the probability that the prognostic effect of HER 2 will be above some useful value (e.g. a HR \> 1.5 for a binary factor, which indicates risk is increased by at least 50%). We can calculate this probability as follows:

$Pr(\mathrm{HR} > 1.5) = Pr(\hat \mu > \log(1.5)) = 1 - Pr(\hat \mu \leq \log(1.5))$

where $Pr(\hat \mu \leq \log(1.5))$ is approximated using a scaled Student-$t$ distribution (similar to the calculation of our prediction interval):

```{r,message=F,warning=F,echo=T,eval=T}
1-pt((log(1.5) - mu)/sqrt(tau2+sigma2), df=(resREML$k-2))
```

```{r,message=F,warning=F,echo=T,eval=T}
probOS <- 1-pt((log(1.5) - mu)/sqrt(tau2+sigma2), df=(resREML$k-2))
```

We can also estimate this probability by means of simulation:

```{r,message=F,warning=F,echo=T,eval=T}
# Simulate 100000 new studies
Nsim <- 1000000 
HRsim <- exp(mu + rt(Nsim, df=(resREML$k-2))*sqrt(tau2+sigma2))
sum(HRsim>1.5)/Nsim
```

`r if(show.text){paste("The probability is ", sprintf("%.0f%%", probOS*100), ", suggesting that using HER2 will often provide adequate discrimination.")}`

## Multivariate meta-analysis

In previous section, we used 14 of the 16 identified studies to evaluate the prognostic effect of HER2 on overall survival. Two studies were excluded from the meta-analysis because they did not provide direct evidence about overall survival. This is unwelcome, especially if the participants are otherwise representative of the population, clinical settings, and condition of interest [@riley_multivariate_2017]. For this reason, we here discuss how multivariate meta-analysis methods can be used to include studies that do not necessarliy provide direct evidence on the outcome of interest. Briefly, multivariate meta-analysis methods simultaneously analyse multiple outcomes, which allows more studies to contribute towards each outcome.

For instance, 6 studies in the review of @zhang_prognostic_2015 assessed the relation between HER2 and PFS. The corresponding hazard ratios are given below:

```{r kablePFS, echo=F}
#table.zhang.PFS
```

We can therefore combine this evidence with the HRs on overall survival to obtain more accurate estimates for the prognostic value of the hormone receptor HER2 with respect to PFS *and* OS.

The hormone receptor HER2 also has prognostic value for progression-free survival. Furthermore, the reported HRs appear to be much more homogeneous across studies.

Perform a random-effects meta-analysis and construct a forest plot.

```{r,message=F,warning=F,echo=T,eval=T}
resPFS <- rma(yi = Zhang$logHR, sei = Zhang$se.logHR, subset = (Zhang$outcome=="PFS"), 
              method = "REML", slab = Zhang$Study)
resPFS
metafor::forest(resPFS, transf = exp, xlab = "Hazard Ratio", refline = 1)
```

Note that the univariate meta-analysis for PFS is based on merely 6 studies, and that the univariate meta-analysis for OS was based on 14 studies. We can now employ multivariate meta-analysis to ''borrow information'' from the 4 studies that report prognostic effects for both PFS and OS (Togami 2012, Mori 2010, Jongen 2009-2 and Backe 1997). This, in turn, allows all studies to contribute on the summary effect for HER2 in both outcomes.

Before we can proceed with the model fitting, we need to construct the full (block-diagonal) variance-covariance for all studies from the two outcomes. We can do this using the `diag()` function in one line of code:

```{r,message=F,warning=F,echo=T,eval=T}
V <- diag(Zhang$se.logHR**2)
```

A multivariate random-effects model can now be used to meta-analyze the two outcomes simultaneously.

```{r,message=F,warning=F,echo=T,eval=T}
res.MV <- rma.mv(yi = logHR, V, mods = ~ outcome - 1, random = ~ outcome | Study, 
                 struct = "UN", data = Zhang, method = "REML")
res.MV
```

`r if(show.text){paste("The new summary estimate is ", sprintf("%.3f", (res.MV$beta["outcomeOS",])), " (versus ", sprintf("%.3f", resREML$b), ") with an SE of ", sprintf("%.3f", sqrt(vcov(res.MV)["outcomeOS","outcomeOS"])), "(versus ", sprintf("%.3f", sqrt(vcov(resREML))), "). Hence, we have gained some precision. ")}`

`r if(show.text){paste("The new summary estimate is ", sprintf("%.3f", (res.MV$beta["outcomePFS",])), " (versus ", sprintf("%.3f", resPFS$b), ") with an SE of ", sprintf("%.3f", sqrt(vcov(res.MV)["outcomePFS","outcomePFS"])), "(versus ", sprintf("%.3f", sqrt(vcov(resPFS))), "). We have lost precision, possibly because the presence of between-study heterogeneity is now more apparent, thereby complicating estimation of the summary effect.")}`

Note that estimation of between-study heterogeneity was difficult for progression-free survival due to the limited number of studies. In particular, we found $\tau^2$= `r sprintf("%.3f", resPFS$tau2)` with an SE of `r sprintf("%.3f", resPFS$se.tau2)`. In the multivariate meta-analysis, the estimated between-study variance for PFS was much larger ($\tau^2$=`r sprintf("%.3f", res.MV$tau2[2])`), and based on all 16 rather than merely 6 studies.

In summary, the multivariate meta-analysis approach is often helpful as it reduces the need to exclude relevant studies from a meta-analysis, thereby decreasing the risk of bias (e.g. due to selective outcome reporting) and potentially improving precision. As indicated by @riley_multivariate_2017, a multivariate meta-analysis of multiple outcomes is most beneficial when the outcomes are highly correlated and the percentage of studies with missing outcomes is large.


# References
